<h1>1. Numba로 시작하는 CUDA Python </h1>

<p>
GPU를 효율적으로 쓰려면 “스레드 계층(그리드·블록·스레드)과 메모리 계층(레지스터·공유·글로벌)”을 이해하고,
작업을 대규모 데이터 병렬로 재구성해야 합니다. 이 글은 Numba를 통해 CUDA 실행 모델을 학습할 때 필요한
핵심 개념을 정리합니다.
</p>

<h2>개념 설명 </h2>
<ul>
  <li><b>CUDA 실행 모델</b>
    <ul>
      <li><b>그리드(Grid)</b>: 동시에 실행되는 블록들의 집합.</li>
      <li><b>블록(Block)</b>: 협업 단위(공유 메모리·동기화 가능). 같은 블록 내 스레드는 <b>__syncthreads()</b>로 배리어 동기화.</li>
      <li><b>스레드(Thread)</b>: 실제 연산 수행 단위. 하드웨어는 <b>워프(일반적으로 32개 스레드)</b> 단위로 스케줄.</li>
      <li><b>실행 구성</b>: <i>gridDim × blockDim</i> 선택이 병렬성·점유율·메모리 패턴을 좌우.</li>
    </ul>
  </li>

  <li><b>Numba의 프로그래밍 모델</b>
    <ul>
      <li><b>@cuda.jit 커널</b>: 호스트에서 런치하는 GPU 함수. 반환값 대신 출력 버퍼를 사용.</li>
      <li><b>디바이스 함수</b>: 커널 내부에서만 호출 가능한 보조 함수.</li>
      <li><b>유니버설 함수(ufunc)</b>: <b>@vectorize(target="cuda")</b> / <b>@guvectorize</b>로 원소별(또는 벡터화) 연산을 자동 병렬화.</li>
      <li><b>배열/전송</b>: <i>to_device, device_array, copy_to_host</i> 등으로 명시적 H2D/D2H 관리.</li>
    </ul>
  </li>

  <li><b>스레드 인덱싱과 스케일링</b>
    <ul>
      <li><b>글로벌 인덱스</b>: <i>cuda.grid(ndim)</i>로 현재 스레드가 처리할 전역 인덱스 계산.</li>
      <li><b>Grid-Stride Loop</b>: 데이터 크기가 크거나 가변일 때, <i>idx += cuda.gridsize(ndim)</i>로 보폭을 두고 순회하는 범용 패턴.</li>
      <li><b>분할 정복</b>: 스레드마다 독립된 부분문제를 맡겨 동기화/경합을 최소화.</li>
    </ul>
  </li>

  <li><b>메모리 계층과 접근 패턴</b>
    <ul>
      <li><b>레지스터</b>: 가장 빠름. 과도 사용 시 점유율 하락·스필(로컬=글로벌) 위험.</li>
      <li><b>공유 메모리</b>: 블록 내 공유되는 온칩 버퍼. <b>타일링</b>으로 재사용 극대화, <b>__syncthreads()</b> 필요.</li>
      <li><b>글로벌 메모리</b>: DRAM. <b>Coalescing</b>(워프가 연속 주소 접근)을 최우선 설계.</li>
      <li><b>상수/읽기 전용 캐시</b>: 불변 파라미터/룩업 테이블에 유리.</li>
    </ul>
  </li>

  <li><b>동기화와 원자성</b>
    <ul>
      <li><b>블록 배리어</b>: <i>__syncthreads()</i>로 공유 버퍼 일관성 확보.</li>
      <li><b>원자 연산</b>: 주소 단위의 순차적 갱신(합계·히스토그램) 보장. 성능-정확성 절충 필요.</li>
      <li><b>계층적 리덕션</b>: 스레드→워프(셔플)→블록(공유)→글로벌 순으로 충돌·트래픽 최소화.</li>
    </ul>
  </li>

  <li><b>스트림과 비동기성(개요)</b>
    <ul>
      <li><b>스트림</b>: 복수 스트림으로 H2D/Compute/D2H를 겹치면 총시간이 <b>최댓값</b>에 수렴.</li>
      <li><b>고정 메모리(pinned)</b>: 진짜 비동기 전송을 위한 호스트 버퍼. 전송 병목 완화.</li>
    </ul>
  </li>

  <li><b>성능 측정과 안정성</b>
    <ul>
      <li><b>비동기 타이밍</b>: 이벤트 기반 측정 + 동기화 필수(워밍업 후 계측).</li>
      <li><b>메모리 오류</b>: 경계 초과·경합·은행 충돌에 주의. 전치/타일 패딩 등으로 예방.</li>
    </ul>
  </li>
</ul>

<h2>실행 결과 (이론적 기대)</h2>
<ul>
  <li><b>Coalescing + 타일링</b>: 글로벌 트래픽 감소로 대역폭 활용 극대화.</li>
  <li><b>Grid-Stride Loop</b>: 문제 크기 변화에 강한 확장성.</li>
  <li><b>계층적 리덕션</b>: 원자 충돌 감소, 동기화 비용 최소화.</li>
  <li><b>오버랩</b>: 스트림·pinned 메모리로 전송과 계산 겹침 → 처리율 향상.</li>
</ul>

<h2>실전 예시 (이론 시나리오)</h2>
<ul>
  <li><b>원소별 수치 변환</b>: ufunc 또는 간단 커널로 대규모 배열 처리.</li>
  <li><b>전치/합성곱/스텐실</b>: 공유 메모리 타일링 + 패딩으로 Coalescing·은행 충돌 최적화.</li>
  <li><b>히스토그램/빈 카운트</b>: 블록 로컬 집계 → 드문 글로벌 원자 갱신.</li>
</ul>

<h2>정리</h2>
<ul>
  <li>실행 구성(<i>grid/block</i>)과 인덱싱(<i>cuda.grid</i>, Grid-Stride)이 병렬화 품질을 좌우합니다.</li>
  <li>Coalescing·타일링·공유 메모리는 메모리 병목을 줄이는 표준 해법입니다.</li>
  <li>동기화(<i>__syncthreads()</i>)와 원자성은 정확성을, 계층적 리덕션은 성능을 보장합니다.</li>
  <li>스트림과 pinned 메모리로 전송과 계산을 겹치면 전체 처리량이 개선됩니다.</li>
</ul>

<h1>2. Numba로 작성하는 커스텀 CUDA 커널</h1>

<h2>개념 설명</h2>

<h3>왜 커스텀 커널인가</h3>
<ul>
  <li><b>ufunc</b>은 스칼라 연산의 <i>원소별</i> 적용에 매우 적합하지만, 모든 병렬 문제를 표현하진 못합니다.</li>
  <li>특정 데이터 접근 패턴, 누적·집계, 의존성이 있는 연산 등은 <b>스레드 계층(그리드/블록/스레드)</b>을 직접 노출하는 <b>커스텀 커널</b>이 더 적합합니다.</li>
  <li>커스텀 커널은 난도가 있지만, 연산·메모리 패턴을 세밀하게 통제할 수 있습니다.</li>
</ul>

<h3>CUDA 커널과 실행 구성</h3>
<ul>
  <li>GPU에서 실행될 함수를 <b>커널</b>이라 부르며, 런치 시 <b>그리드 크기</b>(블록 개수)와 <b>블록 크기</b>(스레드 개수)를 지정합니다.</li>
  <li>각 스레드는 고유한 인덱스(예: 1D에서는 <i>global thread id</i>)로 <b>데이터 영역을 분할</b>해 처리합니다.</li>
  <li>실행 구성 선택은 성능에 직접적인 영향을 주며, 충분한 병렬 작업을 제공해 GPU가 바쁘도록 만드는 것이 핵심입니다.</li>
</ul>

<h3>레이턴시 은닉과 구성 선택의 관점</h3>
<ul>
  <li>GPU는 <b>SM(Stream Multiprocessor)</b> 안에서 <b>워프</b>(일정 수의 스레드 묶음)를 스케줄링하며, 하나의 워프가 대기할 때 다른 워프로 <b>레이턴시를 은닉</b>합니다.</li>
  <li>따라서 <b>적절한 블록/스레드 수</b>로 <b>활성 워프</b>를 충분히 확보해 <b>점유율(occupancy)</b>을 높이는 것이 일반 원칙입니다.</li>
</ul>

<h3>Grid-Stride Loop 패턴</h3>
<ul>
  <li>데이터 크기가 그리드 전체 스레드 개수보다 크거나, 문제 크기가 가변적일 때 쓰는 <b>범용 병렬 처리 패턴</b>입니다.</li>
  <li>각 스레드는 자신의 시작 인덱스에서 시작해, <b>그리드 전체 크기</b>만큼 <b>고정 보폭(stride)</b>으로 건너뛰며 여러 요소를 처리합니다.</li>
  <li>이 패턴은 <b>스케일 아웃</b>에 유리하고, 메모리 접근을 <b>연속(coalesced)</b>되게 유지하는 데 도움을 줍니다.</li>
</ul>

<h3>원자 연산과 레이스 컨디션</h3>
<ul>
  <li>여러 스레드가 동일 메모리 위치를 갱신할 때 <b>읽기-이후-쓰기</b>와 <b>쓰기-이후-쓰기</b> 충돌이 발생할 수 있습니다.</li>
  <li>카운터 증가, 히스토그램 누적처럼 <b>정확한 합계</b>가 필요한 경우 <b>atomic</b> 연산으로 <b>주소 단위의 순차적 갱신</b>을 보장합니다.</li>
  <li>원자 연산은 병렬성을 일부 직렬화하므로, <b>정확성 보장</b>과 <b>성능</b> 사이의 절충이 필요합니다.</li>
</ul>

<h3>성능 계측의 주의점</h3>
<ul>
  <li>커널 런치는 <b>비동기</b>이므로, 정확한 시간 측정을 위해선 적절한 <b>동기화</b>가 필요합니다.</li>
  <li>비교를 위해 <b>CPU 기준선</b>과 <b>단일 스레드 GPU</b> 및 <b>병렬 GPU</b>의 차이를 분리해 보아야 합니다(단일 스레드 GPU는 보통 CPU보다 느립니다).</li>
</ul>

<h3>디버깅/진단 관련 부록 개념</h3>
<ul>
  <li><b>CUDA 시뮬레이터</b>: 실제 GPU 성능과는 무관하지만, 특정 스레드에서 <b>Python 디버거</b>를 사용하는 등 <b>디버깅</b>에 유용한 실행 모드를 제공합니다.</li>
  <li><b>CUDA Memcheck</b>: 잘못된 메모리 접근·경계 초과 등을 탐지해 커널 오류 진단에 활용합니다.</li>
  <li><b>GPU 난수</b>: 스레드별 난수 시드/생성 관리가 필요하며, <i>몬테카를로 π</i> 추정과 같은 예제에 응용됩니다.</li>
</ul>

<h2>실전 예시(이론 시나리오)</h2>
<ul>
  <li><b>대규모 배열의 원소별 수치 연산</b>: Grid-Stride Loop로 확장성 확보, 연속 접근을 유도해 대역폭 활용을 극대화.</li>
  <li><b>히스토그램/카운팅</b>: 병렬 누적 시 <b>원자 연산</b>으로 레이스 방지, 필요하면 단계적 집계로 충돌을 줄임.</li>
  <li><b>몬테카를로 시뮬레이션</b>: 스레드별 난수 샘플링 후 집계(감소 연산), 실행 구성과 레이턴시 은닉으로 처리량 향상.</li>
</ul>

<h2>정리</h2>
<ul>
  <li>커스텀 CUDA 커널은 ufunc로 표현하기 어려운 병렬 문제에 대해 <b>세밀한 제어</b>를 제공합니다.</li>
  <li><b>Grid-Stride Loop</b>는 데이터 크기에 독립적인 <b>확장 가능한 병렬화</b>의 핵심 패턴입니다.</li>
  <li><b>원자 연산</b>은 병렬 누적의 <b>정확성</b>을 보장하며, 성능과의 균형 설계가 필요합니다.</li>
  <li>커널 타이밍은 <b>비동기성</b>을 고려해 <b>동기화</b> 후 측정해야 올바릅니다.</li>
  <li>디버깅·진단을 위해 <b>CUDA 시뮬레이터</b>와 <b>Memcheck</b>를 적절히 활용합니다.</li>
</ul>


<h1>3. CUDA 효과적인 메모리 사용 </h1>

<p>
GPU 성능은 “연산량” 못지않게 “메모리 접근”에서 갈립니다. 동일 커널이라도 접근 패턴을 바꾸면 수배 차이가 납니다.
이 글은 노트북의 핵심 개념(메모리 계층, Coalescing, Shared Memory, Bank Conflict, 캐시·상수 메모리, 레지스터/로컬 스필,
리덕션·원자성, 점유율과 데이터 재사용)을 정리합니다.
</p>

<h2>개념 설명 (정의 + 핵심 규칙)</h2>
<ul>
  <li><b>메모리 계층·원칙</b>
    <ul>
      <li><b>온칩(빠름)</b>: 레지스터 &gt; 공유 메모리(Shared, SM 내부) ≈ L1 캐시</li>
      <li><b>오프칩(느림)</b>: L2 캐시 &gt; 글로벌 메모리(디바이스 DRAM)</li>
      <li><b>원칙</b>: 자주 쓰는 데이터는 가깝게 오래, 멀리 있는 곳은 드물게·연속적으로. <b>재사용/지역성</b>을 키우는 것이 목표.</li>
    </ul>
  </li>

  <li><b>Coalesced Global Access</b>
    <ul>
      <li><b>정의</b>: 같은 워프(32 스레드)가 <b>연속 주소</b>를 한 번에 묶어 읽고/쓰는 것.</li>
      <li><b>규칙</b>: <code>threadIdx.x</code>가 <code>addr + k</code>로 1씩 증가하도록
        데이터 레이아웃/인덱싱을 맞춘다(행·열 순서, 스트라이드 주의).</li>
      <li><b>피해야 할 것</b>: 큰 보폭(stride) 접근, AoS(구조체 배열) 난접근 —
        가능하면 SoA(배열 분리) 및 정렬/패딩으로 트랜잭션 분할을 줄인다.</li>
    </ul>
  </li>

  <li><b>Shared Memory Tiling</b>
    <ul>
      <li><b>정의</b>: 블록 단위 <b>타일</b>을 공유 메모리에 적재해 전역 메모리 왕복을 줄이는 패턴
        (행렬 곱/전치, 스텐실, 합성곱 등).</li>
      <li><b>절차</b>: <b>load → __syncthreads() → compute</b> (필요 시 prefetch 병행).</li>
      <li><b>크기 선택</b>: 재사용 극대화 vs 점유율(occupancy) 저하의 균형.</li>
    </ul>
  </li>

  <li><b>Bank Conflict(공유 메모리)</b>
    <ul>
      <li><b>정의</b>: 같은 사이클에 같은 뱅크를 여러 스레드가 접근하여 직렬화되는 현상.</li>
      <li><b>규칙</b>: 인접 스레드가 서로 다른 뱅크를 치도록 인덱싱.
        전치/타일은 <b>열 패딩(+1)</b>으로 충돌을 완화.</li>
    </ul>
  </li>

  <li><b>읽기 전용/상수 캐시 활용</b>
    <ul>
      <li><b>상수 메모리</b>: 모든 스레드가 동일 주소를 읽을 때 브로드캐스트 유리(작은 파라미터, 룩업 테이블).</li>
      <li><b>읽기 전용 캐시</b>: 불변 데이터(예: 필터, 입력 특성)를 읽기 전용 경로로 접근해 대역폭 절약과 히트율 향상.</li>
    </ul>
  </li>

  <li><b>레지스터와 로컬 메모리(스필)</b>
    <ul>
      <li><b>레지스터</b>: 가장 빠름. 하지만 <b>배열 인덱스가 런타임에 결정</b>되면 로컬(=글로벌)로 스필되어 급격히 느려질 수 있음.</li>
      <li><b>규칙</b>: 상수 인덱싱·언롤링으로 레지스터에 고정. 레지스터 과다 사용은 점유율을 떨어뜨리므로 균형 필요.</li>
    </ul>
  </li>

  <li><b>리덕션·원자 연산</b>
    <ul>
      <li><b>원자(Atomic)</b>: 한 주소의 <b>정확성</b> 보장(합계·히스토그램) — 그러나 <b>직렬화 비용</b> 존재.</li>
      <li><b>권장</b>: <b>계층적 리덕션</b>(스레드→워프→블록→글로벌)으로 원자 충돌 최소화.
        워프 내부는 <b>shuffle</b>로 공유 메모리 없이 합산 가능.</li>
    </ul>
  </li>

  <li><b>전치(Transpose)·스텐실(Stencil) 패턴</b>
    <ul>
      <li><b>전치</b>: 읽기/쓰기 양쪽 Coalescing을 위해 <b>타일 + 패딩</b> 사용.</li>
      <li><b>스텐실</b>: 공유 메모리에 <b>할로(halo)</b> 포함하여 불필요한 전역 재접근 제거.</li>
    </ul>
  </li>

  <li><b>점유율(Occupancy) vs 데이터 재사용(Locality)</b>
    <ul>
      <li><b>오해 금지</b>: 점유율 최댓값이 곧 최고 성능은 아님.</li>
      <li><b>균형</b>: 타일 크기/레지스터/공유 메모리 사용을 조절해
        <b>재사용 극대화</b>와 <b>충분한 병렬성</b> 사이를 맞춘다.</li>
    </ul>
  </li>

  <li><b>호스트↔디바이스 전송 관점(간단)</b>
    <ul>
      <li><b>Pinned Host</b>로 진짜 비동기 전송, 스트림 분리로 H2D/Compute/D2H 오버랩.</li>
      <li>데이터 레이아웃을 전송 친화적으로 구성(큰 연속 블록 단위 I/O).</li>
    </ul>
  </li>
</ul>

<h2>핵심 규칙(암기용)</h2>
<ol>
  <li><b>Coalescing 우선</b>: 워프의 인접 스레드가 인접 주소를 접근하도록 데이터/인덱싱 설계.</li>
  <li><b>공유 메모리 재사용</b>: 타일로 모아 읽고 블록 내 여러 연산에 재사용. 단계 사이 <code>__syncthreads()</code> 필수.</li>
  <li><b>Bank Conflict 회피</b>: 전치/타일에 +1 패딩 등으로 충돌 제거.</li>
  <li><b>레지스터 스필 금지</b>: 인덱싱 상수화·언롤링으로 레지스터 유지.</li>
  <li><b>리덕션은 계층적으로</b>: 스레드→워프(shuffle)→블록(shared)→글로벌 순으로 트래픽/동기화 최소화.</li>
  <li><b>점유율은 수단</b>: “높을수록 무조건 좋다”가 아님. 재사용·대역폭·동기화를 함께 최적화.</li>
  <li><b>읽기 전용/상수 캐시</b>: 변하지 않는 파라미터·룩업은 읽기 전용/상수 경로 활용.</li>
  <li><b>전송도 최적화 범주</b>: Pinned+Streams로 I/O를 겹쳐 전체 파이프라인 효율 확보.</li>
</ol>

<h2>실전 적용 아이디어(이론 시나리오)</h2>
<ul>
  <li><b>행렬 전치</b>: 32×32 타일 + (32×33) 패딩 공유 메모리로 양방향 Coalescing 및 Bank Conflict 회피.</li>
  <li><b>스텐실 계산</b>: 입력 타일+halo를 공유 메모리에 로드 → 여러 출력 셀 생산.</li>
  <li><b>히스토그램/빈 카운트</b>: 블록 로컬 히스토그램 → 전역에는 드문 원자 갱신으로 충돌 감소.</li>
  <li><b>리덕션</b>: 워프 shuffle로 부분 합 → 블록 합 → 최종 합 순서로 동기화·트래픽 최소화.</li>
</ul>

<h2>정리</h2>
<ul>
  <li>GPU 메모리 최적화의 본질은 <b>연속 접근(coalescing)</b>과 <b>재사용(locality)</b> 극대화입니다.</li>
  <li><b>공유 메모리 타일링</b>과 <b>Bank Conflict 회피</b>는 전치/스텐실/합성곱에서 사실상의 표준 전략입니다.</li>
  <li><b>레지스터 관리</b>로 스필을 막고, <b>계층적 리덕션</b>으로 원자 병목을 제거하세요.</li>
  <li><b>점유율</b>은 목표가 아니라 결과물 — 재사용·대역폭·동기화를 함께 고려할 때 최고 성능에 근접합니다.</li>
</ul>
